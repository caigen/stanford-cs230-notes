\newpage{}
\section{Basics of Neural Network Programming}


%============================================
\subsection{Binary Classification}
\subsubsection{Binary Classification}
image $\rightarrow$ 1 (cat) vs 0 (non cat)

\subsubsection{Notation}
%\begin{align}
	$m$: number of examples

	$n_x$: input size

	$n_y$: output size

	$x$: input, column vector

	$y$: output, 0/1

	$X$: input matrix, shape = $(n_x, m)$

	$Y$: output matrix, shape = $(1, m)$

	$x^{(i)}$: superscript (i) will denote the $i^{th}$ example.
%\end{align}


%============================================
\subsection{Logistic Regression}
Given:
	$x \in \mathbb{R}^{n_x}$, $0 \le \hat{y} \le 1$

Parameters: 
	$w \in \mathbb{R}^{n_x}$, $b \in \mathbb{R}$

Output: 
\begin{align}
	z = w^Tx + b   \\
	\hat{y} = \sigma{(z)} \\
	\sigma{(z)} \approx \frac{1}{1 + e^{-z}} \\
	z \approx \infty, \sigma{(z)} \approx \frac{1}{1 + 0} = 1 \\
	z \approx -\infty, \sigma{(z)} \approx \frac{1}{1 + \infty} = 0
\end{align}

Simplified Parameters:
	$x_0 = 1$, $x \in \mathbb{R}^{{n_x} + 1}$

\begin{align}
	\theta_0 = b, \theta_1 ... \theta_{n_x} = w \\
	\theta = \begin{bmatrix} 
				\theta_0 \\
				\theta_1 \\
				\theta_2 \\
				...      \\
				\theta_{n_x} \\
	         \end{bmatrix} \\
	\hat{y} = \sigma{(\theta^Tx)}
\end{align}


%============================================
\subsection{Logistic Regression cost function}

Given $\{ (x^{(1)}, y^{(1)}, ..., (x^{(m)}, y^{(m)} \}$,
want $\hat{y}^{(i)} \approx {y}^{(i)}$.

Loss (error) function (mean square error/cross entropy):
\begin{align}
	\mathcal{L}(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2  \\
	\mathcal{L}(\hat{y}, y) = -(y\log{\hat{y}} + (1-y)\log(1 - \hat{y}))
\end{align}

If y = 1: $\mathcal{L}(\hat{y}, y) = -\log{\hat{y}}$, 
want $\mathcal{L}$ small, want $\hat{y}$ large, want $\hat{y}$ equal to 1.  
If y = 0: $\mathcal{L}(\hat{y}, y) = -\log{(1 - \hat{y})}$,
want $\mathcal{L}$ small, want $\hat{y}$ small, want  $\hat{y}$ equal to 0. 


Cost function:
\begin{align}
	J(w, b) = \frac{1}{m} \displaystyle\sum_{i=1}^m \mathcal{L}(\hat{y}, y) \\
	J(w, b) = - \frac{1}{m} \displaystyle\sum_{i=1}^m [y^{(i)}\log{\hat{y}^{(i)}} + (1 - y^{(i)})\log{1 - \hat{y}^{(i)}}]
\end{align}

Given a random variable $X$ with probability mass function $p_X(x)$, 
the self information of measuring $X$ as outcome $x$ is defined asï¼š
\begin{align}
	I_X(x) = \log[p_X(x)] = \log(\frac{1}{p_X(x)})
\end{align}

Shannon Entroy of $X$:
\begin{align}
	H(X) = \sum_{x} -p_X(x)\log{p_X(x)} \\
	= \sum_{x} p_X(x)I_X(x) \\
	= E[I_X(x)]
\end{align}

Cross Entropy of the the true distributions $p$ and estimated distribution $q$:
\begin{align}
	H(p, q) = E_p[-\log{q}] = -\sum_{x \in \mathcal X} p(x) \log{q(x)}
\end{align}


%============================================
\subsection{Gradient Descent}
Want to find w, b that minimize $J(w, b)$.

Repeat:
\begin{align}
	w := w - \alpha \frac{\mathrm{d} J(w,b)}{\mathrm{d} w} \\
	b := b - \alpha \frac{\mathrm{d} J(w,b)}{\mathrm{d} b}
\end{align}

$\alpha$: learning rate


%============================================
\subsection{Derivatives}
\begin{align}
	f(a) = 3a, \frac{\mathrm{d} f(a)}{\mathrm{d} a} = 3 = \frac{\mathrm{d}}{\mathrm{d} a} f(a) \\
	f(a) = a^2, \frac{\mathrm{d}}{\mathrm{d} a} f(a) = 2a \\
	f(a) = a^3, \frac{\mathrm{d}}{\mathrm{d} a} f(a) = 3a^2 \\
	f(a) = \log_e{a} = \ln{a}, \frac{\mathrm{d}}{\mathrm{d} a} f(a) = \frac{1}{a} \\
	f(x) = \log_a{x}, \frac{\mathrm{d}}{\mathrm{d} x} f(x) = \frac{1}{x\ln{a}} \\
	f(x) = a^x, \frac{\mathrm{d}}{\mathrm{d} x} f(x) = a^x \ln{a} \\
	\log_a{b} = \frac{\log_c{b}}{\log_c{a}} = \frac{\ln{a}}{\ln{b}}
\end{align}