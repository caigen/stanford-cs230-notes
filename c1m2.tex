\newpage{}
\section{Basics of Neural Network Programming}


%============================================
\subsection{Binary Classification}
\subsubsection{Binary Classification}
image $\rightarrow$ 1 (cat) vs 0 (non cat)

\subsubsection{Notation}
%\begin{align}
	$m$: number of examples

	$n_x$: input size

	$n_y$: output size

	$x$: input, column vector

	$y$: output, 0/1

	$X$: input matrix, shape = $(n_x, m)$

	$Y$: output matrix, shape = $(1, m)$

	$x^{(i)}$: superscript (i) will denote the $i^{th}$ example.
%\end{align}


%============================================
\subsection{Logistic Regression}
Given:
	$x \in \mathbb{R}^{n_x}$, $0 \le \hat{y} \le 1$

Parameters: 
	$w \in \mathbb{R}^{n_x}$, $b \in \mathbb{R}$

Output: 
\begin{align}
	z = w^Tx + b   \\
	\hat{y} = \sigma{(z)} \\
	\sigma{(z)} \approx \frac{1}{1 + e^{-z}} \\
	z \approx \infty, \sigma{(z)} \approx \frac{1}{1 + 0} = 1 \\
	z \approx -\infty, \sigma{(z)} \approx \frac{1}{1 + \infty} = 0
\end{align}

Simplified Parameters:
	$x_0 = 1$, $x \in \mathbb{R}^{{n_x} + 1}$

\begin{align}
	\theta_0 = b, \theta_1 ... \theta_{n_x} = w \\
	\theta = \begin{bmatrix} 
				\theta_0 \\
				\theta_1 \\
				\theta_2 \\
				...      \\
				\theta_{n_x} \\
	         \end{bmatrix} \\
	\hat{y} = \sigma{(\theta^Tx)}
\end{align}


%============================================
\subsection{Logistic Regression cost function}

Given $\{ (x^{(1)}, y^{(1)}, ..., (x^{(m)}, y^{(m)} \}$,
want $\hat{y}^{(i)} \approx {y}^{(i)}$.

Loss (error) function (mean square error/cross entropy):
\begin{align}
	\mathcal{L}(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2  \\
	\mathcal{L}(\hat{y}, y) = -(y\log{\hat{y}} + (1-y)\log(1 - \hat{y}))
\end{align}

If y = 1: $\mathcal{L}(\hat{y}, y) = -\log{\hat{y}}$, 
want $\mathcal{L}$ small, want $\hat{y}$ large, want $\hat{y}$ equal to 1.  
If y = 0: $\mathcal{L}(\hat{y}, y) = -\log{(1 - \hat{y})}$,
want $\mathcal{L}$ small, want $\hat{y}$ small, want  $\hat{y}$ equal to 0. 


Cost function:
\begin{align}
	J(w, b) = \frac{1}{m} \displaystyle\sum_{i=1}^m \mathcal{L}(\hat{y}, y) \\
	J(w, b) = - \frac{1}{m} \displaystyle\sum_{i=1}^m [y^{(i)}\log{\hat{y}^{(i)}} + (1 - y^{(i)})\log{1 - \hat{y}^{(i)}}]
\end{align}

Given a random variable $X$ with probability mass function $p_X(x)$, 
the self information of measuring $X$ as outcome $x$ is defined asï¼š
\begin{align}
	I_X(x) = \log[p_X(x)] = \log(\frac{1}{p_X(x)})
\end{align}

Shannon Entroy of $X$:
\begin{align}
	H(X) &= \sum_{x} -p_X(x)\log{p_X(x)} \\
	&= \sum_{x} p_X(x)I_X(x) \\
	&= E[I_X(x)]
\end{align}

Cross Entropy of the the true distributions $p$ and estimated distribution $q$:
\begin{align}
	H(p, q) = E_p[-\log{q}] = -\sum_{x \in \mathcal X} p(x) \log{q(x)}
\end{align}


%============================================
\subsection{Gradient Descent}
Want to find w, b that minimize $J(w, b)$.

Repeat:
\begin{align}
	w := w - \alpha \frac{\mathrm{d} J(w,b)}{\mathrm{d} w} \\
	b := b - \alpha \frac{\mathrm{d} J(w,b)}{\mathrm{d} b}
\end{align}

$\alpha$: learning rate


%============================================
\subsection{Derivatives}
\begin{align}
	f(a) = 3a, \frac{\mathrm{d} f(a)}{\mathrm{d} a} = 3 = \frac{\mathrm{d}}{\mathrm{d} a} f(a) \\
	f(a) = a^2, \frac{\mathrm{d}}{\mathrm{d} a} f(a) = 2a \\
	f(a) = a^3, \frac{\mathrm{d}}{\mathrm{d} a} f(a) = 3a^2 \\
	f(a) = \log_e{a} = \ln{a}, \frac{\mathrm{d}}{\mathrm{d} a} f(a) = \frac{1}{a} \\
	f(x) = \log_a{x}, \frac{\mathrm{d}}{\mathrm{d} x} f(x) = \frac{1}{x\ln{a}} \\
	f(x) = a^x, \frac{\mathrm{d}}{\mathrm{d} x} f(x) = a^x \ln{a} \\
	\log_a{b} = \frac{\log_c{b}}{\log_c{a}} = \frac{\ln{a}}{\ln{b}}
\end{align}


%============================================
\subsection{Computation Graph}
\begin{align}
	J(a, b, c) &= 3(a + bc) \\
	&= 3(a + u) \\
	&= 3v \\
	u &= bc \\
	v &= a + u \\
	J &= 3v 
\end{align}


%============================================
\subsection{Derivatives with a Computation Graph}
a = 5, b = 3, c = 2
\begin{align}
	\frac{\mathrm{d}J}{\mathrm{d}v} &= 3 \\
	\frac{\mathrm{d}J}{\mathrm{d}a} &= \frac{\mathrm{d}J}{\mathrm{d}v} \frac{\mathrm{d}v}{\mathrm{d}a} \\
	&= 3 * 1 \\
	&= 3 \\
	\frac{\mathrm{d}J}{\mathrm{d}u} &= \frac{\mathrm{d}J}{\mathrm{d}v} \frac{\mathrm{d}v}{\mathrm{d}u} \\ 
	&= 3 \\
	\frac{\mathrm{d}J}{\mathrm{d}b} &= \frac{\mathrm{d}J}{\mathrm{d}u} \frac{\mathrm{d}u}{\mathrm{d}b} \\
	&= 3 * c \\
	&= 3 * 2 \\
	&= 6 \\
	\frac{\mathrm{d}J}{\mathrm{d}c} &= \frac{\mathrm{d}J}{\mathrm{d}u} \frac{\mathrm{d}u}{\mathrm{d}c} \\
	&= 3 * b \\
	&= 3 * 3 \\
	&= 9
\end{align}


%============================================
\subsection{Logistic Regression Gradient Descent}
\subsubsection{Logicstic regression recap}
\begin{align}
	&z = w^Tx + b \\
	&\hat{y} = a = \sigma(z) \\
	&\mathcal{L}(a, y) = -(y\log(a) + (1-y)\log(1 - a))
\end{align}

\subsubsection{Logistic regression derivatives}
$z = w_1 x_1 + w_2 x_2 + b \rightarrow a = \sigma(z) \rightarrow \mathcal L (a, y)$
\begin{align}
	\mathrm{d} a &= \frac{\mathrm{d} \mathcal{L}(a, y)} {\mathrm{d} a} \\
					&= -\frac{y}{a} + \frac{1 - y}{1 - a} \\
	\mathrm{d} z &= \frac{\mathrm{d} \mathcal{L}(a, y)} {\mathrm{d} z} \\
			      &= \frac{\mathrm{d} \mathcal{L}} {\mathrm{d} a} * \frac{\mathrm{d} a} {\mathrm{d} z} \\
					&= (-\frac{y}{a} + \frac{1 - y}{1 - a}) * a * (1 - a) \\
				   &= -(1-a)y + a(1-y) \\
                &= a - y \\
	\frac{\mathrm{d} a} {\mathrm{d} z} &= a * (1 - a) \\
		&= \frac{1}{1 + e^{-z}} * (1 - \frac{1}{1 + e^{-z}}) \\
	\mathrm{d} w_1 &= x_1 * \mathrm{d} z \\
	\mathrm{d} w_2 &= x_2 * \mathrm{d} z \\
	\mathrm{d} b &= \mathrm{d} z
\end{align}


Repeat:
\begin{align}
	w_1 &:= w_1 - \alpha \mathrm{d} w_1 \\
	w_2 &:= w_2 - \alpha \mathrm{d} w_2 \\
	  b &:= b - \alpha \mathrm{d} b
\end{align}


%============================================
\subsection{Gradient Descent on $m$ examples}
\begin{align}
	J(w, b) &= \frac{1}{m} \sum_{i = 1}^{m} \mathcal{L}(a^{(i)}, y{(i)}) \\
	\frac{\mathrm{d}} {\mathrm{d}w_1} J(w, b) &= \frac{1}{m} \sum_{i = 1}^{m} 
		\frac{\mathrm{d}} {\mathrm{d}w_1} \mathcal{L}(a^{(i)}, y{(i)}) \\
	&= \frac{1}{m} \sum_{i = 1}^{m} \mathrm{d}w_1^{(i)}
\end{align}


%============================================
\subsection{Vectorization}
Vectorized, for GPU.
\begin{lstlisting}[language={python},tabsize=4]
	z = np.dot(w, x) + b
\end{lstlisting}


%============================================
\subsection{More Vectorization Examples}
\subsubsection{Neural network programming guideline}
Whenever possible, avoid explicit for-loops.
\begin{lstlisting}[language={python},tabsize=4]
	u = np.dot(A, x)
\end{lstlisting}

\subsubsection{Vectors and matrix valued functions}
\begin{lstlisting}[language={python},tabsize=4]
	import numpy as np
	
	u = np.exp(v)
	np.log(v)
	np.abs(v)
	np.maximum(v, 0)
	np.pow(v, 2)
	np.divide(1, v)
\end{lstlisting}


%============================================
\subsection{Vectorizing Logistic Regression}


%============================================
\subsection{Vectorizing Logistic Regression's Gradient Computation}


%============================================
\subsection{Broadcasting in Python}
\begin{lstlisting}[language={python},tabsize=4]
	np.add((1.0, 2.0), 2.0)   # (3.0, 4.0)
\end{lstlisting}


%============================================
\subsection{Explanation of logistic regression cost function (Optional)}
\subsubsection{Logistic regression cost function}
$\hat{y} = \sigma(w^Tx + b)$ where $\sigma(z) = \frac{1}{1 + e^{-z}}$
\begin{align}
	\hat{y} = p(y = 1 | x)
\end{align}


If $y = 1$: $p(y|x) = \hat{y}$

If $y = 0$: $p(y|x) = 1 - \hat{y}$

\begin{align}
	p(y|x) &= \hat{y}^y (1-\hat{y})^{(1-y)} \\
	y = 1: p(y|x) &= \hat{y} \\
	y = 0: p(y|x) &= 1 - \hat{y} \\
	\log_{}{p(y|x)} &= y\log_{}{\hat{y}} + (1-y)\log_{}{1 - \hat{y}} \\
					   &= -\mathcal{L}(\hat{y}, y)
\end{align}


\subsubsection{Cost on $m$ examples}
Maximum likehood estimation:
\begin{align}
	\log_{}{p(...)} &= \sum_{i = 1}^{m} -\mathcal{L}(\hat{y}, y) \\
					   &= -\sum_{i = 1}^{m} \mathcal{L}(\hat{y}, y)
\end{align}

Minimize cost:
\begin{align}
	J(w, b) = \frac{1}{m} \displaystyle\sum_{i=1}^m \mathcal{L}(\hat{y}, y)
\end{align}
